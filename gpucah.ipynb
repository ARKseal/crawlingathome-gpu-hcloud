{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    " \n",
    "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
    "print(\"CUDA version:\", CUDA_version)\n",
    " \n",
    "if CUDA_version == \"10.0\":\n",
    "    torch_version_suffix = \"+cu100\"\n",
    "elif CUDA_version == \"10.1\":\n",
    "    torch_version_suffix = \"+cu101\"\n",
    "elif CUDA_version == \"10.2\":\n",
    "    torch_version_suffix = \"+cu101\"\n",
    "else:\n",
    "    torch_version_suffix = \"+cu110\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone \"https://github.com/TheoCoombes/crawlingathome\" crawlingathome_client\n",
    "! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html\n",
    "! pip3 install -r crawlingathome_client/requirements.txt --no-cache-dir\n",
    "! rm requirements.txt\n",
    "! wget https://raw.githubusercontent.com/rvencu/crawlingathome-gpu-hcloud/main/requirements.txt\n",
    "! wget https://raw.githubusercontent.com/rvencu/crawlingathome-gpu-hcloud/main/cloud-config.yaml\n",
    "! pip3 install -r ./requirements.txt --no-cache-dir\n",
    "! pip3 install datasets ftfy pandas tfr_image trio\n",
    "! pip3 install tensorflow --no-cache-dir\n",
    "! pip3 install git+https://github.com/openai/CLIP --no-cache-dir\n",
    "#! yes | pip3 uninstall pillow\n",
    "#! CC=\"cc -mavx2\" pip3 install -U --force-reinstall pillow-simd\n",
    "\n",
    "! git clone \"https://github.com/hetznercloud/hcloud-python\" hcloud\n",
    "! pip3 install -e ./hcloud\n",
    "\n",
    "! yes | rm cloud-init\n",
    "! cp cloud-config.yaml cloud-init\n",
    "\n",
    "! yes | ssh-keygen -t rsa -b 4096 -f $HOME/.ssh/id_cah -q -P \"\"\n",
    "! sed -i -e \"s/<<your_ssh_public_key>>/$(sed 's:/:\\\\/:g' ~/.ssh/id_cah.pub)/\" cloud-init"
   ]
  },
  {
   "source": [
    "Now please restart the runtime then continue with the next cell !"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title GPU controlled Hetznet Cloud swarm of workers\n",
    "YOUR_NICKNAME_FOR_THE_LEADERBOARD = \"Colab-GPU-hcloud\" #@param {type:\"string\"}\n",
    "HCLOUD_API_TOKEN = \"<<your_hcloud_api_token>>\" #@param {type:\"string\"}\n",
    "SWARM_NODES = 10 #@param {type:\"string\"}\n",
    "\n",
    "#input file\n",
    "with open(\"cloud-init\", \"rt\") as init:\n",
    "    string = init.read()\n",
    "string = string.replace('<<your_nickname>>', YOUR_NICKNAME_FOR_THE_LEADERBOARD)\n",
    "with open(\"cloud-init\", \"wt\") as init:\n",
    "    init.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import trio\n",
    "import clip\n",
    "import torch\n",
    "import pipes\n",
    "import string\n",
    "import random\n",
    "import pickle\n",
    "import shutil\n",
    "import zipfile\n",
    "import datasets\n",
    "import itertools\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from itertools import cycle\n",
    "from anyascii import anyascii\n",
    "sys.path.append('./crawlingathome-worker/')\n",
    "from PIL import ImageFile, UnidentifiedImageError\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # https://stackoverflow.com/a/47958486\n",
    "import PIL.Image\n",
    "\n",
    "from hcloud import Client\n",
    "from hcloud.images.domain import Image\n",
    "from hcloud.hcloud import APIException\n",
    "from hcloud.server_types.client import ServerType\n",
    "\n",
    "output_folder = \"./save/\"\n",
    "csv_output_folder = output_folder\n",
    "img_output_folder = output_folder + \"images/\"\n",
    "\n",
    "nodes = sys.argv[1]\n",
    "workers = []"
   ]
  },
  {
   "source": [
    "# define CLIP class around OpenAI clip model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "datasets.set_caching_enabled(False)\n",
    "\n",
    "class CLIP:\n",
    "    def __init__(self):\n",
    "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.cosine_similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        self.categories = self.model.encode_text(clip.tokenize([\"neutral\",\"selfie\", \"illustration, drawing\", \"toys, play, kids, children\", \"teddy bear, puppet\", \"animal, bird, mammal, insect\" \"fashion, clothes\", \"logo, commercial, ad, advertisement\", \"drawing, painting\",\"anime, cartoon\",\"comedy, fun\",\"romance, love story\",\"thriller, suspense, crime story\",\"action, action movie\", \"horror, monster movie\", \"documentary\", \"news, journalism\", \"entertainment\", \"talk show\", \"porn, sex, sperm, nipples, breats, tits, boops, penis, dick, cock, clitoris, vagina, fuck, lust, horny, sexual, lick, licking\",  \"porn, sex, sperm, nipples\", \"porn, sex, sperm, penis, dick, cock\", \"nipples, breats, tits, boops, sexy\", \"penis, dick, cock\", \"clitoris, vagina\", \"sex, fuck, lust, horny, sexual, lick, licking\", \"porn, sex, sexy\",\"sexy, hot\",\"sperm, skin\",\"lust, horny, sexual\",\"lick, licking, body\", \"anime, hentai, sexy\", \"cartoon, sexy, sex\", \"hentai\", \"anime, sexy, breasts\", \"hentai\"]).to(device))\n",
    "        self.underaged_categories = self.model.encode_text(clip.tokenize([\"teenager, teen\", \"kid, child, teenager, teen, baby or toddler, underaged, little girl, little boy\", \"kid, child, little girl, little boy\", \"baby, toddler\",\"adult, woman, man, grownup, grown person,full-aged of legal age\",\"full-aged, of legal age, adult\",\"woman, man\",\"adult, woman, man, grownup, grown person,full-aged of legal age\"]).to(device))\n",
    "        self.animal_categories = self.model.encode_text(clip.tokenize([\"lifeless object, thing\", \"thing, object\", \"material\", \"furniture\",\"wall\", \"house\", \"tree\", \"wood\",\"ground\",\"industry\", \"table\", \"bed\", \"tool\", \"dress, clothes\", \"door\", \"chair\", \"rock, stone\", \"human\", \"man\", \"woman\", \"man, woman\", \"animal\",\"cat\",\"dog\", \"cow\", \"pig\", \"goat\", \"sheep\", \"elephant\", \"horse\", \"horse, elephant, pig, dog, cat, sheep, goat, animal\", \"life\", \"wildlife\"]).to(device))\n",
    "\n",
    "\n",
    "    def similarity_imgalt(self, batch):\n",
    "        similarity = []\n",
    "        images = [\n",
    "            self.preprocess(PIL.Image.open(path)).unsqueeze(0).to(device)\n",
    "            for path in batch[\"PATH\"]\n",
    "        ]\n",
    "        max_texts = [anyascii(text)[:77] for text in batch[\"TEXT\"]]\n",
    "        texts = clip.tokenize(max_texts).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.encode_image(\n",
    "                torch.cat(images)\n",
    "            ).float()\n",
    "            text_features = self.model.encode_text(texts).float()\n",
    "\n",
    "        for image_feat, text_feat in zip(image_features, text_features):\n",
    "            similarity.append(\n",
    "                float(\n",
    "                    self.cosine_similarity(\n",
    "                        torch.reshape(text_feat, (1, 512)),\n",
    "                        torch.reshape(image_feat, (1, 512)),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        batch[\"similarity\"] = similarity\n",
    "        batch[\"image_features\"] = image_features.detach().cpu().numpy()\n",
    "        return batch\n",
    "\n",
    "    def preprocess_images(self, df):\n",
    "        im_dataset = datasets.Dataset.from_pandas(df)\n",
    "        im_dataset = im_dataset.map(self.similarity_imgalt, batched=True, batch_size=512)\n",
    "        return im_dataset[\"image_features\"], im_dataset[\"similarity\"]\n",
    "\n",
    "    def prob(self, image_features, text_features):\n",
    "        image_features = torch.as_tensor(image_features).to(device)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        similarity = (100.0 * image_features.float() @ text_features.T.float()).softmax(dim=-1)\n",
    "        _, indices = similarity.topk(2)\n",
    "        return indices\n"
   ]
  },
  {
   "source": [
    "# define inference utility functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipfolder(filename, target_dir):            \n",
    "    zipobj = zipfile.ZipFile(filename, 'w', zipfile.ZIP_DEFLATED)\n",
    "    rootlen = len(target_dir) + 1\n",
    "    for base, dirs, files in os.walk(target_dir):\n",
    "        for file in files:\n",
    "            fn = os.path.join(base, file)\n",
    "            zipobj.write(fn, fn[rootlen:])\n",
    "\n",
    "def df_clipfilter(df):\n",
    "    sim_threshold = 0.3\n",
    "    underaged_text = [\"teen\", \"kid\", \"child\", \"baby\"]\n",
    "\n",
    "    clip = CLIP()\n",
    "    img_embedding, similarities = clip.preprocess_images(df)\n",
    "    tmp_embed = copy(img_embedding)\n",
    "    for i, img_embed in enumerate(tmp_embed):\n",
    "        if similarities[i] < sim_threshold:\n",
    "            df.drop(i, inplace=True)\n",
    "            img_embedding.remove(img_embed)\n",
    "            continue\n",
    "\n",
    "        # get most similar categories\n",
    "        nsfw_prob = clip.prob(img_embed, clip.categories)\n",
    "        df.at[i, \"NSFW\"] = \"UNSURE\"\n",
    "        df.at[i, \"similarity\"] = similarities[i]\n",
    "        if nsfw_prob[0] < 19 and nsfw_prob[1] < 19:\n",
    "            df.at[i, \"NSFW\"] = \"UNLIKELY\"\n",
    "            continue\n",
    "        elif nsfw_prob[0] >= 19 and nsfw_prob[1] >= 19:\n",
    "            df.at[i, \"NSFW\"] = \"NSFW\"\n",
    "\n",
    "        underage_prob = clip.prob(img_embed, clip.underaged_categories)\n",
    "        if (\n",
    "            underage_prob[0] < 4\n",
    "            or underage_prob[1] < 4\n",
    "            or any(x in df.at[i, \"TEXT\"] for x in underaged_text)\n",
    "        ):\n",
    "            df.drop(i, inplace=True)\n",
    "            img_embedding.remove(img_embed)\n",
    "            continue\n",
    "\n",
    "        animal_prob = clip.prob(img_embed, clip.animal_categories)\n",
    "        if animal_prob[0] > 20:\n",
    "            df.drop(i, inplace=True)\n",
    "            img_embedding.remove(img_embed)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df, img_embedding\n",
    "\n",
    "\n",
    "def df_tfrecords(df, output_fname):\n",
    "    import tensorflow as tf\n",
    "    from tfr_image.utils import bytes_feature, int64_feature\n",
    "\n",
    "    def image_to_tfexample(sample_id, image_data, image_format, height, width, caption):\n",
    "        return tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    \"sampleID\": bytes_feature(sample_id),\n",
    "                    \"image\": bytes_feature(image_data),\n",
    "                    \"format\": bytes_feature(image_format),\n",
    "                    \"label\": bytes_feature(caption),\n",
    "                    \"height\": int64_feature(height),\n",
    "                    \"width\": int64_feature(width),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_fname) as tfrecord_writer:\n",
    "        for i in range(len(df)):\n",
    "            df_image = df.iloc[i]\n",
    "            image_fname = df_image[\"PATH\"]\n",
    "            file_type = image_fname.split(\".\")[-1]\n",
    "            with tf.io.gfile.GFile(image_fname, \"rb\") as f:\n",
    "                image_data = f.read()\n",
    "            example = image_to_tfexample(\n",
    "                str(df_image[\"SAMPLE_ID\"]).encode(\"utf_8\"),\n",
    "                image_data,\n",
    "                file_type.encode(\"utf_8\"),\n",
    "                df_image[\"HEIGHT\"],\n",
    "                df_image[\"WIDTH\"],\n",
    "                df_image[\"TEXT\"].encode(\"utf_8\"),\n",
    "            )\n",
    "            tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# define swarm management tools"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists_remote(host, path, silent=False):\n",
    "    \"\"\"Test if a file exists at path on a host accessible with SSH.\"\"\"\n",
    "    status = subprocess.call(\n",
    "        [\"ssh\", \"-oStrictHostKeyChecking=no\", \"-oIdentitiesOnly=yes\", \"-i~/.ssh/id_cah\", host, \"test -f {}\".format(pipes.quote(path))],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL,\n",
    "    )\n",
    "    if not silent:\n",
    "        print(\".\", end = \"\", flush=True)\n",
    "    if status == 0:\n",
    "        return True\n",
    "    if status == 1 or status == 255:\n",
    "        return False\n",
    "\n",
    "async def list_servers():\n",
    "    hclient = Client(token=HCLOUD_API_TOKEN.strip())\n",
    "    return hclient.servers.get_all()\n",
    "\n",
    "async def swarm_up(server_type=\"cx11\"):\n",
    "    workers = []\n",
    "    hclient = Client(token=HCLOUD_API_TOKEN.strip())\n",
    "    locations = hclient.locations.get_all()\n",
    "    loc = cycle(locations)\n",
    "    zip = [[i, next(loc)] for i in range(int(SWARM_NODES))]\n",
    "    with open(\"cloud-init\", \"r\") as user_data:\n",
    "        script = user_data.read()\n",
    "        for i, loc in zip:\n",
    "            try:\n",
    "                response = hclient.servers.create(\n",
    "                    \"cah-worker-\"+str(i),\n",
    "                    ServerType(name=server_type),\n",
    "                    Image(name=\"ubuntu-20.04\"),\n",
    "                    hclient.ssh_keys.get_all(),\n",
    "                    None, #volumes\n",
    "                    None, #firewalls\n",
    "                    None, #networks\n",
    "                    script,\n",
    "                    None, #labels\n",
    "                    loc, #location - todo: create servers in all locations\n",
    "                    None, #datacenter\n",
    "                )\n",
    "                srv = response.server\n",
    "                workers.append(srv.public_net.ipv4.ip)\n",
    "            except APIException as e:\n",
    "                print (f\"[swarm] API Exception: \" + str(e))\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        print (f\"[swarm] Swarm intialized with {len(workers)} nodes. If this is less than expected please check your account limits\")\n",
    "        return workers\n",
    "\n",
    "async def swarm_down():\n",
    "    servers = await list_servers(token)\n",
    "    hclient = Client(token=HCLOUD_API_TOKEN.strip())\n",
    "    for server in servers:\n",
    "        server = hclient.servers.get_by_name(server.name)\n",
    "        if server is None:\n",
    "            continue\n",
    "        server.delete()\n",
    "\n",
    "async def wait_for_swarm (workers):\n",
    "    print(f\"[swarm] Waiting for {len(workers)} nodes to become ready\")\n",
    "    for i in range(len(workers)):\n",
    "        subprocess.call(\n",
    "            [\"ssh-keygen\", \"-R\", workers[i]],\n",
    "            stdout=subprocess.DEVNULL,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        )\n",
    "\n",
    "        while not exists_remote(\n",
    "            f\"crawl@{workers[i]}\", \"/home/crawl/crawl.log\"\n",
    "        ):\n",
    "            time.sleep(30)\n",
    "\n",
    "async def node_respawn(workers, ip, server_type=\"cx11\"):\n",
    "    hclient = Client(token=HCLOUD_API_TOKEN.strip())\n",
    "    index = workers.index(ip)\n",
    "    server = hclient.servers.get_by_name(f\"cah-worker-{index}\")\n",
    "    if server is None:\n",
    "        return\n",
    "    try:\n",
    "        # first attempt to restart the crawl service\n",
    "        subprocess.call(\n",
    "            [\"ssh\", \"-oIdentitiesOnly=yes\", \"-i~/.ssh/id_cah\", \"crawl@\" + ip, \"sudo\", \"systemctl\", \"restart\", \"crawl\"],\n",
    "            stdout=subprocess.DEVNULL,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        )\n",
    "    except:\n",
    "        # if impossible to restart the service then delete the worker and try to re-create it\n",
    "        server.delete()\n",
    "        with open(\"cloud-init\", \"r\") as user_data:\n",
    "            script = user_data.read()\n",
    "            try:\n",
    "                response = hclient.servers.create(\n",
    "                    \"cah-worker-\"+index,\n",
    "                    ServerType(name=server_type),\n",
    "                    Image(name=\"ubuntu-20.04\"),\n",
    "                    hclient.ssh_keys.get_all(),\n",
    "                    None, #volumes\n",
    "                    None, #firewalls\n",
    "                    None, #networks\n",
    "                    script,\n",
    "                    None, #labels\n",
    "                    None, #location - todo: create servers in all locations\n",
    "                    None, #datacenter\n",
    "                )\n",
    "                srv = response.server\n",
    "                workers[index] = srv.public_net.ipv4.ip\n",
    "            except APIException as e:\n",
    "                # problem. we remove the worker from the dispatcher\n",
    "                print (f\"[swarm] API Exception: \" + str(e))\n",
    "                workers.remove(ip)\n",
    "                return workers\n",
    "    return workers\n",
    "\n",
    "def node_status(host,path):\n",
    "    read = subprocess.run(\n",
    "        [\"ssh\", \"-oStrictHostKeyChecking=no\", \"-oIdentitiesOnly=yes\", \"-i~/.ssh/id_cah\", host, \"tail -1 {}\".format(pipes.quote(path))],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    return read.stdout"
   ]
  },
  {
   "source": [
    "# launching swarm. Be patient... Next cell might take 10 minutes to complete"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    start = time.time()\n",
    "    # generate cloud workers\n",
    "    workers = trio.run(swarm_up, int(SWARM_NODES))\n",
    "    trio.run(wait_for_swarm, workers)\n",
    "    print(f\"[swarm] {len(workers)} nodes swarm is up and initialized in {round(time.time() - start)}s\")\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"[swarm] Abort! Deleting swarm...\")\n",
    "    trio.run(swarm_down)\n",
    "    print (f\"[swarm] Swarm was shutdown\")\n",
    "except Exception as e:\n",
    "    print(f\"[swarm] Error, could not bring up swarm... please consider shutting down all workers manually in the cloud console\")\n",
    "    print (e)"
   ]
  },
  {
   "source": [
    "# process incoming inference jobs and monitor swarm nodes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# poll for new GPU job\n",
    "for ip in itertools.cycle(workers): # make sure we cycle all workers\n",
    "    try:\n",
    "        print (f\"[GPU] Checking {ip} node\")\n",
    "        print (f\"[{ip}] \" + node_status(\"crawl@\"+ip, '/home/crawl/crawl.log').split(\"Downloaded:\")[-1].rstrip())\n",
    "        newjob = exists_remote(\"crawl@\"+ip, \"/home/crawl/semaphore\", True)\n",
    "        if not newjob:\n",
    "            time.sleep(10) # wait until cloud-init finishes then until jobs are ready for GPU\n",
    "        else:\n",
    "            start = time.time()\n",
    "\n",
    "            print (f\"[{ip}] sending job to GPU\")\n",
    "            if os.path.exists(output_folder):\n",
    "                shutil.rmtree(output_folder)\n",
    "            if os.path.exists(\".tmp\"):\n",
    "                shutil.rmtree(\".tmp\")\n",
    "\n",
    "            os.mkdir(output_folder)\n",
    "            os.mkdir(img_output_folder)\n",
    "            os.mkdir(\".tmp\")\n",
    "\n",
    "            # receive gpu job data (~500MB)\n",
    "            subprocess.call(\n",
    "                [\"scp\", \"-oIdentitiesOnly=yes\", \"-i~/.ssh/id_cah\", \"crawl@\" + ip + \":\" + \"gpujob.zip\", output_folder],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL,\n",
    "            )\n",
    "            # delete file on remote so there is no secondary download\n",
    "            subprocess.call(\n",
    "                [\"ssh\", \"-oIdentitiesOnly=yes\", \"-i~/.ssh/id_cah\", \"crawl@\" + ip, \"rm -rf gpujob.zip\"],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL,\n",
    "            )\n",
    "            subprocess.call(\n",
    "                [\"ssh\", \"-oIdentitiesOnly=yes\", \"-i~/.ssh/id_cah\", \"crawl@\" + ip, \"rm -rf semaphore\"],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL,\n",
    "            )\n",
    "            with zipfile.ZipFile(output_folder+\"gpujob.zip\", 'r') as zip_ref:\n",
    "                zip_ref.extractall(\"./\")\n",
    "            os.remove(output_folder+\"gpujob.zip\")\n",
    "\n",
    "            all_csv_files = []\n",
    "            for path, subdir, files in os.walk(output_folder):\n",
    "                for file in glob(os.path.join(path, \"*.csv\")):\n",
    "                    all_csv_files.append(file)\n",
    "\n",
    "            # get name of csv file\n",
    "            out_path = all_csv_files[0]\n",
    "            out_fname = Path(out_path).stem.strip(\"_unfiltered\").strip(\".\")\n",
    "            print (out_fname)\n",
    "\n",
    "            # recreate parsed dataset and run CLIP filtering\n",
    "            dlparse_df = pd.read_csv(output_folder + out_fname + \".csv\", sep=\"|\")\n",
    "            filtered_df, img_embeddings = df_clipfilter(dlparse_df)\n",
    "            filtered_df.to_csv(output_folder + out_fname + \".csv\", index=False, sep=\"|\")\n",
    "            \n",
    "            img_embeds_sampleid = {}\n",
    "            for i, img_embed_it in enumerate(img_embeddings):\n",
    "                dfid_index = filtered_df.at[i, \"SAMPLE_ID\"]\n",
    "                img_embeds_sampleid[str(dfid_index)] = img_embed_it\n",
    "            with open(f\"{output_folder}image_embedding_dict-{out_fname}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(img_embeds_sampleid, f)\n",
    "            \n",
    "            df_tfrecords(\n",
    "                filtered_df,\n",
    "                f\"{output_folder}crawling_at_home_{out_fname}__00000-of-00001.tfrecord\",\n",
    "            )\n",
    "\n",
    "            # clean img_output_folder now since we have all results do not want to transfer back all images...\n",
    "            try:\n",
    "                shutil.rmtree(img_output_folder)\n",
    "                os.mkdir(img_output_folder)\n",
    "            except OSError as e:\n",
    "                print(\"[GPU] Error deleting images: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "            # send GPU results\n",
    "            subprocess.call(\n",
    "                [\"zip\", \"-r\", \"gpujobdone.zip\", output_folder],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL,\n",
    "            )\n",
    "            subprocess.call(\n",
    "                [\"touch\", \"gpusemaphore\"],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL,\n",
    "            )\n",
    "\n",
    "            subprocess.call(\n",
    "                [\"scp\", \"-oIdentitiesOnly=yes\", \"-i~/.ssh/id_cah\", \"gpujobdone.zip\", \"crawl@\"+ip + \":~/gpujobdone.zip\"],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL,\n",
    "            )\n",
    "            subprocess.call(\n",
    "                [\"scp\", \"-oIdentitiesOnly=yes\", \"-i~/.ssh/id_cah\", \"gpusemaphore\", \"crawl@\"+ip + \":~/gpusemaphore\"],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL,\n",
    "            )\n",
    "            os.remove(\"gpujobdone.zip\")\n",
    "            os.remove(\"gpusemaphore\")\n",
    "\n",
    "            print(f\"[GPU] GPU job completed in {round(time.time() - start)} seconds\")\n",
    "            print (f\"[{ip}] resuming job with GPU results\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"[GPU] Abort! Deleting cloud infrastructure...\")\n",
    "        letters = string.ascii_lowercase\n",
    "        suffix = ''.join(random.choice(letters) for i in range(3))\n",
    "        for ip in workers:\n",
    "            subprocess.call(\n",
    "                    [\"scp\", \"-oIdentitiesOnly=yes\", \"-i~/.ssh/id_cah\", \"crawl@\" + ip + \":\" + \"crawl.log\", ip + \"_\" + suffix + \".log\"],\n",
    "                    stdout=subprocess.DEVNULL,\n",
    "                    stderr=subprocess.DEVNULL,\n",
    "                )\n",
    "        trio.run(swarm_down)\n",
    "        print (f\"[swarm] Cloud infrastructure was shutdown\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # todo shutdown and restart the offending ip\n",
    "        print (f\"[GPU] fault detected in job at worker-{ip}. Respawning offending worker...\")\n",
    "        print (e)\n",
    "        workers = trio.run(node_respawn, workers, ip)\n",
    "        continue\n"
   ]
  },
  {
   "source": [
    "## in case colab disconnected or crashed, do not let the swarm alive, it will continue to consume cash"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trio.run(swarm_down)\n",
    "print (f\"[swarm] Cloud infrastructure was shutdown\")"
   ]
  },
  {
   "source": [
    "## inspect any swarm node\n",
    "util commands\n",
    "\n",
    "`! ssh -oStrictHostKeyChecking=no -oIdentitiesOnly=yes -i~/.ssh/id_cah crawl@<<ip_here>>`\n",
    "\n",
    "once inside\n",
    "\n",
    "`tail -f crawl.log`\n",
    "\n",
    "`sudo systemctl restart crawl`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh -oStrictHostKeyChecking=no -oIdentitiesOnly=yes -i~/.ssh/id_cah crawl@<<ip_here>>"
   ]
  },
  {
   "source": [
    "how to get cloud-init logs for a node that fails to install proprly"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! scp -oStrictHostKeyChecking=no -oIdentitiesOnly=yes -i~/.ssh/id_cah crawl@<<ip_here>>:/var/log/cloud-init-output.log cloud-init-<<ip_here>>.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('gpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "1ba613d68b3b18985e02519c8e1689c7243aaf59a45b2d39d56345edb6b92440"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}